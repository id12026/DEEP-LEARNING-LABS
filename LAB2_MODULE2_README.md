# LAB2_MODULE 2
# Customer Churn Prediction using ANN

This repository contains two implementations of an **Artificial Neural Network (ANN)** for predicting customer churn.

## ğŸ“Œ Overview
Customer churn prediction is an essential task for businesses to retain customers and reduce revenue loss. I have implement **two versions of an ANN** using TensorFlow/Keras:

1. **Baseline Model (Simple ANN)**
2. **Regularized Model (with L2 Regularization, Dropout & Batch Normalization)**

---

## ğŸš€ Dataset
We use a customer churn dataset (`Churn_Modelling.csv`) containing the following key features:
- **Categorical Variables**: `Geography`, `Gender`
- **Numerical Variables**: `CreditScore`, `Age`, `Balance`, etc.
- **Target Variable**: `Exited` (1 = Churn, 0 = Retained)

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ baseline_model.py  # ANN with only input and output layers
â”œâ”€â”€ regularized_model.py  # ANN with L2 Regularization, Dropout & BatchNorm
â”œâ”€â”€ churn_data.csv  # Sample dataset (not included here)
â”œâ”€â”€ README.md  # This file
```

---

## ğŸ“Œ 1: Baseline ANN (No Regularization)
This simple ANN contains:
- **1 Input Layer**
- **1 Output Layer**
- **Sigmoid Activation Function** for binary classification

### **ğŸ”¹ Code Snippet**
```python
# Build ANN model (Baseline)
model = Sequential([
    Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))
])
```
âœ… **Optimizer:** Adam  
âœ… **Loss Function:** Binary Crossentropy  
âœ… **Metric:** Accuracy  

### **ğŸ”¹ Performance**
- No overfitting control  
- Might struggle with complex patterns in data  

---

## ğŸ“Œ : Regularized ANN (L2, Dropout & BatchNorm)
This model is improved with:
- **L2 Regularization (`l2(0.01)`)** to reduce overfitting  
- **Dropout (`Dropout(0.3)`)** to randomly deactivate neurons  
- **Batch Normalization** to stabilize learning  

### **ğŸ”¹ Code Snippet**
```python
# Build ANN model with Regularization
model = Sequential([
    Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    BatchNormalization()
])
```
âœ… **Prevents overfitting**  
âœ… **Stabilizes learning & improves generalization**  

---

## ğŸ“Š Performance Comparison

| Model  | Regularization | Accuracy  | Overfitting Risk |
|--------|--------------|-----------|-----------------|
| Baseline ANN | âŒ None | Moderate | High |
| Regularized ANN | âœ… L2, Dropout, BatchNorm | Higher | Low |

---

## ğŸ“ˆ Visualizations
### **ğŸ”¹ Confusion Matrix**
Both models generate a **confusion matrix** to evaluate predictions:

![Confusion Matrix](confusion_matrix.png)

### **ğŸ”¹ Accuracy & Loss Graphs**
Models also plot:
1. **Training vs. Validation Accuracy**
2. **Training vs. Validation Loss**

---

## ğŸ› ï¸ Installation & Usage
### **ğŸ“Œ 1ï¸âƒ£ Install Dependencies**
```bash
pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras
```

### **ğŸ“Œ 2ï¸âƒ£ Run the Models**
- **Run the Baseline Model**
  ```bash
  python baseline_model.py
  ```
- **Run the Regularized Model**
  ```bash
  python regularized_model.py
  ```

---

## ğŸ† Results & Insights
1. The **regularized model** performs better than the baseline model.
2. **Dropout & L2 regularization** help prevent overfitting.
3. **Batch Normalization** stabilizes the training process.

---


ğŸ”— **Developed by:** [Mohitha Bandi_22WU0105037_DS-B](https://github.com/id12026)  
ğŸ“§ **E-mail:** mohitha12026@gmail.com
```

---

### **ğŸ“Œ Key Features of this README**
âœ… **Well-structured & clear**  
âœ… **Explains both models**  
âœ… **Shows performance comparison**  
âœ… **Includes installation & usage steps**  


Would you like me to customize it further? ğŸš€
